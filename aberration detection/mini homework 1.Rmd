---
title: "hw1"
author: "Dan Weinberger"
date: "February 6, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(surveillance)
library(lubridate)
library(zoo)
```

```{r chile.import, include=FALSE}
#ch1<-read.csv('C:/Users/dmw63/Desktop/My documents h/TEACHING/chile data/chile01_11.csv')

#ch.aids<-ch1[substr(ch1$diag1,1,3) %in% c('B20','Z21'),]
#write.csv(ch.aids,'C:/Users/dmw63/Desktop/My documents h/TEACHING/chile data/chile_aids.csv')
#ch.aids<-read.csv('C:/Users/dmw63/Desktop/My documents h/TEACHING/chile data/chile_aids.csv')
```

##Instructions for this homework exercise.

You should complete this assignment alone. Feel free to consult class materials and the internet as needed. When you are finished, submit this .Rmd file. The coding steps you need to format the dataset into time series and explore the data are completed for you. You will need to fiddle with the aberration detection schemes to adjust sensitivity to an appropriate level and justify your decisions. There are 5 questions.

##Goal for the analysis
Your goal is to set up an aberration detection system for hospitalizations associated with HIV in Chile. 

EDAD: age in years
diag1: primary diagnosis, in ICD10 coding
date: Date admitted to hospital 

You have been provided with a subset of the larger database that includes ICD10 codes B20 and Z21, which are codes for HIV/AIDS-related causes of hospitalization: https://www.icd10data.com/ICD10CM/Codes/A00-B99

### First read in the data 
Change the directory as needed to point to where the .csv file is saved
```{r readscsv}
d1<-read.csv('C:/Users/dmw63/Desktop/My documents h/TEACHING/chile data/chile_aids.csv')
```

## Tell R that 'date' is a date variable and assign a format
see https://www.statmethods.net/input/dates.html
```{r date_format}
d1$date<- as.Date(d1$date, "%d%b%Y")
```

Then create a new variablecalled 'week.date' that has the date of the Sunday of the week in which the admission occurred. The lubridate has some functions that make this easier. You want to round the date down to the nearest Sunday. The floor_data function can accomplish this: https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf

```{r date_format2}
#"round" the date down to 
d1$week.date<-floor_date(d1$date, unit='week')
```


### Then do some basic explorations. What is the distibution of ages? of Dates? (make a histogram for each)
```{r hist1}
hist(d1$EDAD, xlab='Age (years)' )
hist(d1$date, breaks=10)
```

### Frequency of the codes

# QUESTION 1: 
Using an online ICD10 dictionary, what are the top 3 diagnosis codes used for HUV/AIDS in this database? 

ZZZZZ

```{r freq.codes, echo=FALSE}
sort(table(d1$diag1),decreasing=T)
```

Unlike the in-class exercise that we completed, we are not going t extract subsets of codes--we will use all HIV-related codes for our case definition.
```{r}
icd10.3digits<-substr(d1$diag1,1,3) #extract 1st 3 digits from IC10 code
icd10.3digits[1:10] #view first 10

#Initialize variables
#d1$a00_a09<-rep(0, nrow(d1))
#d1$a00_a09[icd10.3digits %in% c('A00', 'A01', 'A02', 'A03', 'A04', 'A05', 'A06', 'A07', 'A08', 'A09') ]<-1
d1$hiv <- 1
#table(d1$a00_a09, d1$diag1) #Ceeck your work

```

## Let's aggregate now by week.date

```{r}
d1.split<-split(d1, d1$week.date)
d2<- sapply(d1.split, function(x) sum(x$hiv) )
```

It is also important to make sure that the time series is 'filled'--if there are weeks with 0 counts, it needs to be represented in the time series. to do this, let's make a vector of week.dates and see if it is of the length as our data
```{r}
dates<-as.Date(sort(names(d2))) #Etract and sort the weekdates
date.seq <- seq.Date(from=dates[1], to=dates[length(dates)], by='week') #create a sequence from first to last week date
length(dates)==length(date.seq) #test if the time series is complete

#data frame 1 has just the time series with all weeks represented 
date.seq<-as.data.frame(date.seq)
names(date.seq)<-'date' #make sure column name is 'date'

#Data frame 2 to have the counts and a Date variable
d3<-as.data.frame(d2)
d3$date<-as.Date(row.names(d3))

d4<- merge(d3, date.seq, all=T, by='date')
names(d4)<-c('date', 'hiv')

d4$hiv[is.na(d4$hiv)] <- 0


d5<-d4

#write.csv(d5,'./aberration detection/d5 hiv.csv') #can save a copy of the dataset if you want

```

*Dataset d5 will be used in all further analyses. If you got stuck prior to this, go ahead and read in and use the pre-formatted dataset 'd5.csv'*
```{r}
#d5<- read.csv('./aberration detection/d5 hiv.csv') #Only run this if you were not able to complete previous steps
```


```{r}
plot(d5$date, d5$hiv, type='l', bty='l', ylab='Cases HIV')
```

## Now let's format the data for the surveillance package
Need to determine the year and week of the first data. Can use week(date) and year(date) functions to do this

```{r}
year.start<-lubridate::year(d5$date)[1]
week.start<-week(d5$date)[1]
hiv_DP <- create.disProg(
      week = 1:nrow(d5), #Index of observations
      observed = d5$hiv ,
      state=matrix(0, nrow=nrow(d5), ncol=1),
      start = c(year.start, week.start))
```

## Aberration detection algorithms

First try to use historical limits for the weeks in 2007

# QUESTION 2

The default settings evaluate a evaluate 1 4-week period on either side of the current 4 week period (window size of 3) in each of the previous 5 years. How does the threshold change if you instead include 2 or 3 4-week periods on either side (window size of 5 or 7)? What would be an advantage or disdvantge of using a wider window with these data?

ZZZ

```{r hist.limits}
test.index<-which(lubridate::year(d5$date) %in% c(2007)) #returns vector of the eek numbers for 2008 and 2009

hl2<-algo.cdc(hiv_DP,control = list(b = 5,  #N years in baseline
                                               m = 1, #N 4 week periods on either side
                                              alpha=0.05, #Signifiance level
                                               range=test.index)) #weeks tested

#This stuff just makes a pretty plot
col.alarms<-c(rep(1, times=(hl2$control$range[1]-1)) , (hl2$alarm+2))
cols<-c('gray', 'black', 'red')
all.agg<-rollapply(hiv_DP$observed,4,sum, align='right', fill=NA)
plot(all.agg , pch=16 , bty='l', ylab='Cases', xlab='Week', col=cols[col.alarms])
points(c(rep(NA, times=(hl2$control$range[1]-1)) , hl2$upperbound), type='l')
title('Historical limits Cases vs threshold: alarms are RED')

```


##Farrington method

# Question 3
Fiddle around with b (how many years of historical data to use (try 3-6)), w (how many weeks in each of historical periods to use on ether side of current week number (try 3-25)),reweight (downweight past epidemics?), and alpha (significance value, try 0.001-0.05). What would be an optimal set of parameters going forward? 

ZZZZ

```{r farrington1, echo=TRUE}
test.index<-which(lubridate::year(d5$date) %in% c(2008, 2009,2010,2011)) #returns vector of the eek numbers for 2007 and 2008

mod1<-algo.farrington(hiv_DP, #dataset to use
                control=list(range=test.index,
                b=5, #How many years of historical data to use (must be < or = number of years before 2008)
                w=26, #Number of weeks before and after current week to include in    baseline fitting                             model fitting
                reweight=TRUE, #Do you want to downweight past epidemics? TRUE or FALSE
                plot=FALSE,
                alpha=0.01
                ))

col.alarms<-c(rep(1, times=(mod1$control$range[1])-1) , (mod1$alarm+2))
cols<-c('gray', 'black', 'red')
plot(d5$date,mod1$disProgObj$observed , pch=16 , bty='l', ylab='Cases', xlab='Week', col=cols[col.alarms])
points(d5$date[1:max(mod1$control$range)],c(rep(NA, times=(mod1$control$range[1]-1)) , mod1$upperbound), type='l')
title('Farrington. Cases vs threshold: alarms are RED')
```




## UNADJUSTED CUSUM

Let's first determine some estimates for k and h  
```{r , fig.width=6, fig.height=8}
in.control.mean<- mean(hiv_DP$observed[1:104]) #choose some of the observations to calculate non-epidemic mean
ARL.set <- 104 #aHow many weeks do you want to go (on average) without false alarm?
epidemic.increase=1.5 #how big of an increase do you want to detect? ie a 2-fold increase

##Ask R for values of K and H thresholds based on criteria set above
theta1 <- in.control.mean*epidemic.increase
s1=(theta1-in.control.mean)/sqrt(in.control.mean)
optimized.parms<-findH(ARL0=ARL.set, theta0=in.control.mean, s = 1, rel.tol = 0.03, roundK = FALSE, distr = c("poisson"))
k.optimized<- optimized.parms['k']
h.optimized<- optimized.parms['h']
```


```{r,fig.width=6, fig.height=8}
time.points.test<-which(lubridate::year(d5$date) %in% c(2004:2011)) #returns vector of the week numbers for the indicted years

cusum1<-algo.cusum(hiv_DP, 
                    control = list(range=time.points.test,  #time index of time points to test
                                  k = k.optimized , 
                                  h = h.optimized, 
                                  trans = "none", 
                                  alpha = NULL ))

#PLOT THE OUTPUT
par(mfrow=c(2,1))
plot(cusum1$disProgObj$observed[time.points.test], bty='l',type='l')
abline(h= k.optimized )
title(paste0('Observed data with K=',round(k.optimized,2), ', ARL=', ARL.set, ' weeks'))

plot(cusum1$cusum, type='p', bty='l',pch=16, col=cusum1$alarm+1)
abline(h= h.optimized)
title(paste0('CUSUM statistic with H=', round(h.optimized,2),' red=ALARM'))

```

In the last few years, there is a trend in the data; does using the 'glm' option in algo.cusum help? (note the GLM option uses all previous time points to fit the regression model that adjusts for trend and seasonality)

```{r,, fig.width=6, fig.height=8}
time.points.test<-which(lubridate::year(d5$date) %in% c(2009:2011)) #returns vector of the week numbers for the indicted years
k.set=9.8
h.set=9.4

cusum2<-algo.cusum(hiv_DP, 
                    control = list(range=time.points.test,  #time index of time points to test
                                  k = k.set , 
                                  h =h.set,
                                  trans = "none", 
                                  m='glm',
                                  alpha = NULL ))

#Plotting...
m.plot<-c(rep(NA, times=(cusum2$control$range[1]-1)),cusum2$control$m)
cusum.plot<-c(rep(NA, times=(cusum2$control$range[1]-1)),cusum2$cusum)
alarm.vec<- c(rep(1, times=(cusum2$control$range[1]-1)),cusum2$alarm+2 )
col.alarm.vec=c('gray', 'black','red')

par(mfrow=c(2,1))
plot(cusum2$disProgObj$observed, bty='l',type='p', ylab='Observed cases',col=col.alarm.vec[alarm.vec], pch=16)
#abline(h= k.optimized )
points(m.plot, type='l', col='gray')
title(paste0('Observed data',k.set))

plot(cusum.plot, type='p', bty='l',pch=16, col=col.alarm.vec[alarm.vec])
abline(h= h.set)
title(paste0('CUSUM statistic with H=', h.set,' red=ALARM'))
```
## Question 4
The default settings above clearly don't work well. How could we (1) decrease the sensitivity of the system or (2) get a better estimate for epected cases?